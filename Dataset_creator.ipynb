{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DISCRIMINATOR OF EOA AND CONTRACT ADDRESSES *********************\n",
    "\n",
    "# We checked for some of eoa_addresses and found all of them was eoa addresses (but we can run for all remaining ones)\n",
    "import requests \n",
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_if_contract(address):\n",
    "    url = \"https://mainnet.infura.io/v3/8f890b3a78e740f2bd98be613da634f1\"  # URL of your Ethereum node\n",
    "\n",
    "    payload = {\n",
    "        \"method\": \"eth_getCode\",\n",
    "        \"params\": [address, \"latest\"],\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\"\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    result = response.json()['result']\n",
    "\n",
    "    return result != '0x'\n",
    "\n",
    "# Open the CSV file and read it into memory\n",
    "with open('dataset/eoa_addresses.csv', 'r') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    headers = next(reader)  # Extract header row\n",
    "    data = list(reader)\n",
    "\n",
    "# Add the 'is_eoa' column if it doesn't exist\n",
    "if 'is_eoa' not in headers:\n",
    "    headers.append('is_eoa')\n",
    "    for row in data:\n",
    "        row.append('')  # Initialize with an empty string or any other default value\n",
    "\n",
    "# Find the index of the is_eoa column\n",
    "is_eoa_index = headers.index('is_eoa')\n",
    "\n",
    "# Open the output file\n",
    "with open('dataset/checked_eoa_addresses.csv', 'w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(headers)  # Write the header row\n",
    "\n",
    "    # Iterate over the data and check the is_eoa value for each address\n",
    "    for row in tqdm(data, desc=\"Processing addresses\"):\n",
    "        if row[is_eoa_index]:  # Skip if is_eoa is already populated\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        # Check if address is a contract\n",
    "        is_contract = check_if_contract(row[0])\n",
    "        # Update is_eoa field in the row\n",
    "        row[is_eoa_index] = 0 if is_contract else 1\n",
    "        # Write the updated row to the output file\n",
    "        writer.writerow(row)\n",
    "\n",
    "# prev progress bar: Processing addresses:   0%|          | 56/84234 [01:17<32:22:51,  1.38s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************* DONWLOAD CONTRACTS FROM DB ************************\n",
    "\n",
    "# Download the contracts collection directly from MongoDB\n",
    "client = MongoClient('mongodb://seshatadmin:uWBOzDTQLXJLiFFF@lg-research-1.uwaterloo.ca:8094/')\n",
    "db = client['test']\n",
    "collection = db['contracts']\n",
    "\n",
    "# Retrieve all documents in the collection\n",
    "results = collection.find()\n",
    "\n",
    "# Convert the cursor to a list of dictionaries\n",
    "documents = list(results)\n",
    "\n",
    "# Save the documents as JSON in a file\n",
    "with open('dataset/contracts.json', 'w') as file:\n",
    "    json.dump(documents, file)\n",
    "\n",
    "print(\"Data saved as contracts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data into a pandas DataFrame\n",
    "with open('dataset/transactions.json') as f:\n",
    "    # data = [json.loads(line) for line in f]\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "column_names = df.columns.to_list()\n",
    "print(column_names)\n",
    "\n",
    "columns_to_delete = [col for col in df.columns if col.startswith('func_args')]\n",
    "df = df.drop(columns=columns_to_delete)\n",
    "\n",
    "columns_to_keep = ['from', 'to']\n",
    "df = df.drop(columns=df.columns.difference(columns_to_keep))\n",
    "\n",
    "df['from'] = df['from'].astype(str)\n",
    "df['to'] = df['to'].astype(str)\n",
    "\n",
    "# Get unique Ethereum public keys from 'from' and 'to' columns\n",
    "unique_addresses = np.unique(np.concatenate([df['from'].unique(), df['to'].unique()]))\n",
    "\n",
    "print(len(unique_addresses))\n",
    "\n",
    "# Load the contracts.json file\n",
    "with open('dataset/contracts.json') as file:\n",
    "    contracts_data = json.load(file)\n",
    "\n",
    "# Create a set of contract addresses for faster lookup\n",
    "contract_addresses = set(contract['contractAddress'] for contract in contracts_data)\n",
    "\n",
    "# Create empty arrays for EOA accounts (rows) and contract accounts (columns)\n",
    "eoa_accounts = []\n",
    "contract_accounts = []\n",
    "\n",
    "# Iterate over unique addresses and categorize them\n",
    "for address in tqdm(unique_addresses, desc=\"Categorizing Addresses\"):\n",
    "    if address in contract_addresses:\n",
    "        contract_accounts.append(address)\n",
    "    else:\n",
    "        eoa_accounts.append(address)\n",
    "\n",
    "address_df = pd.DataFrame(unique_addresses, columns=['Address'])\n",
    "\n",
    "# Save as a CSV file\n",
    "address_df.to_csv('unique_addresses.csv', index=False)\n",
    "\n",
    "print(\"Unique addresses saved as unique_addresses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EOA ADDRESSES DATASET BUILDER ##################\n",
    "\n",
    "'''\n",
    "The script to go over unique addresses, lookup within contract_addresses, \n",
    "If not exist, append to the EoA_addresses and save the csv\n",
    "'''\n",
    "\n",
    "# Read unique addresses from unique_addresses.csv\n",
    "unique_addresses = set()\n",
    "with open('dataset/unique_addresses.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        address = row['Address']\n",
    "        unique_addresses.add(address)\n",
    "\n",
    "# Read contract addresses from contract_addresses.csv\n",
    "contract_addresses = set()\n",
    "with open('dataset/contract_addresses.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    total_lines = sum(1 for _ in file)  # Count total lines in the file\n",
    "    file.seek(0)  # Reset file position\n",
    "    progress_bar = tqdm(reader, total=total_lines, desc=\"Processing Contract Addresses\")\n",
    "    for row in progress_bar:\n",
    "        address = row['address']\n",
    "        contract_addresses.add(address)\n",
    "\n",
    "# Find EOA addresses\n",
    "eoa_addresses = unique_addresses - contract_addresses\n",
    "\n",
    "# Append EOA addresses to eoa_addresses.csv\n",
    "fieldnames = ['address']\n",
    "with open('dataset/eoa_addresses.csv', 'a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    if file.tell() == 0:\n",
    "        writer.writeheader()\n",
    "    for address in eoa_addresses:\n",
    "        writer.writerow({'address': address})\n",
    "\n",
    "print(\"EOA addresses saved to eoa_addresses.csv.\")\n",
    "print(len(eoa_addresses)) # 84234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PRE-RUN SCRIPT FOR TX_FETCHER>PY ###################\n",
    "\n",
    "'''\n",
    "When stop tx_fetcher, re-run these four steps before running the script again\n",
    "Step0: merging user-tx dataset json files\n",
    "Step1: To extract all the top level keys (addresses) from new fetched txs\n",
    "Step2: just delete the latest one and rename the new_... to latest_..., To create the local_remaining_addresses, \n",
    "just finding all addresses that don't exist in processed.csv and exists in remaining.csv\n",
    "Step 3: update user-tx csv dataset\n",
    "'''\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory = \"temp_tx\"\n",
    "\n",
    "# Initialize an empty dictionary to store the merged data\n",
    "merged_data = {}\n",
    "\n",
    "# Initialize a counter for skipped files\n",
    "skipped = 0\n",
    "\n",
    "# Initialize a list to store the paths of successfully processed files\n",
    "processed_files = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            try:\n",
    "                data_list = json.load(file)\n",
    "                \n",
    "                # Iterate over each dictionary in the list\n",
    "                for data in data_list:\n",
    "                    # Merge the data into the merged_data dictionary\n",
    "                    merged_data.update(data)\n",
    "                \n",
    "                # Add the path of the successfully processed file to the list\n",
    "                processed_files.append(file_path)\n",
    "            except ValueError as e:\n",
    "                # If an error occurs, increment the skipped counter and continue\n",
    "                skipped += 1\n",
    "                print(f\"Skipping file {filename} due to error: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "print(f\"Skipped {skipped} files.\")\n",
    "\n",
    "# Write the merged data to a new file\n",
    "output_file = \"dataset/merged_user_transactions.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(merged_data, file, indent=4)\n",
    "\n",
    "# Delete the successfully processed files\n",
    "for file_path in processed_files:\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step1\n",
    "#To extract all the top level keys (addresses) from new fetched txs\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# specify the directory you want to parse json files from\n",
    "directory = 'dataset'\n",
    "filename = 'merged_user_transactions.json'\n",
    "\n",
    "\n",
    "with open(os.path.join(directory, filename), 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "addresses = data_list.keys()\n",
    "\n",
    "# If you want to convert it to a list\n",
    "addresses_list = list(addresses)\n",
    "\n",
    "# Write the list of addresses to a csv file\n",
    "with open('dataset/processed_eoa_addresses.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['address'])\n",
    "    for address in addresses_list:\n",
    "        writer.writerow([address])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2, TODO: just delete the latest one and rename the new_... to latest_...\n",
    "#To create the local_remaining_addresses, just finding all addresses that don't exist in processed.csv and exists in remaining.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from csv files\n",
    "remaining_addresses_df = pd.read_csv('dataset/latest_remaining_eoa_addresses.csv')\n",
    "processed_addresses_df = pd.read_csv('dataset/processed_eoa_addresses.csv')\n",
    "\n",
    "# Find addresses that are in addresses.csv but not in remaining_addresses.csv\n",
    "difference_df = remaining_addresses_df.loc[~remaining_addresses_df['address'].isin(processed_addresses_df['address'])]\n",
    "\n",
    "# Save these addresses to a new csv file\n",
    "difference_df.to_csv('dataset/new_remaining_eoa_addresses.csv', index=False)\n",
    "\n",
    "# delete the latest_remaining_eoa_addresses, and rename the new_remaining_... to latest_remaining_...\n",
    "os.remove(\"dataset/latest_remaining_eoa_addresses.csv\") \n",
    "os.rename(\"dataset/new_remaining_eoa_addresses.csv\", \"dataset/latest_remaining_eoa_addresses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "# update user-tx csv dataset\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the merged data\n",
    "with open(\"dataset/merged_user_transactions.json\", \"r\") as file:\n",
    "    merged_data = json.load(file)\n",
    "\n",
    "# Open a new CSV file for appending\n",
    "with open(\"dataset/user_transactions.csv\", \"a\", newline='') as file:\n",
    "    # Define the fieldnames for the CSV\n",
    "    fieldnames = [\"address\", \"timeStamp\", \"from\", \"to\", \"value\", \"gas\", \"gasPrice\", \"input\", \"contractAddress\", \"methodId\", \"functionName\"]\n",
    "    \n",
    "    # Create a CSV writer\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # If file doesn't exist, write the header row\n",
    "    if os.stat(\"dataset/user_transactions.csv\").st_size == 0:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Initialize a counter for skipped addresses\n",
    "    skipped_addresses = 0\n",
    "    \n",
    "    # Iterate over the merged data\n",
    "    for address, data in merged_data.items():\n",
    "        # Check if data[\"normal\"] is a list\n",
    "        if isinstance(data, dict) and \"normal\" in data and isinstance(data[\"normal\"], list):\n",
    "            # For each address, iterate over the transactions\n",
    "            for tx in data[\"normal\"]:\n",
    "                # Create a row for each transaction\n",
    "                row = {\n",
    "                    \"address\": address,\n",
    "                    \"timeStamp\": tx.get(\"timeStamp\", \"\"),\n",
    "                    \"from\": tx.get(\"from\", \"\"),\n",
    "                    \"to\": tx.get(\"to\", \"\"),\n",
    "                    \"value\": tx.get(\"value\", \"\"),\n",
    "                    \"gas\": tx.get(\"gas\", \"\"),\n",
    "                    \"gasPrice\": tx.get(\"gasPrice\", \"\"),\n",
    "                    \"input\": tx.get(\"input\", \"\"),\n",
    "                    \"contractAddress\": tx.get(\"contractAddress\", \"\"),\n",
    "                    \"methodId\": tx.get(\"methodId\", \"\"),\n",
    "                    \"functionName\": tx.get(\"functionName\", \"\")\n",
    "                }\n",
    "                \n",
    "                # Write the row to the CSV\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            # Increment the counter of skipped addresses\n",
    "            skipped_addresses += 1\n",
    "\n",
    "# Print the number of skipped addresses\n",
    "print(f\"Skipped addresses: {skipped_addresses}\")\n",
    "\n",
    "# Remove the temp merged_tx dataset \n",
    "os.remove(\"dataset/merged_user_transactions.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          address  count\n",
      "0      0x0000000000000000000000000000000000000000     50\n",
      "1      0x000000000000000000000000000000000000dead     50\n",
      "2      0x00000000000357848314f068feca5d42e878a1d9     50\n",
      "3      0x000000000074b24153d2d44e9d1beb308d8a1eb6     50\n",
      "4      0x0000000048429ba5463a4a9aa866460087dcebd0     50\n",
      "...                                           ...    ...\n",
      "50646  0xfffe695cb09f056e0ddfdd8f0e447037610c945b     50\n",
      "50647  0xffff14106945bcb267b34711c416aa3085b8865f     50\n",
      "50648  0xffff3dcb664c3f69b049d121fba7b7d7273961ef     50\n",
      "50649  0xffff46e05a09314daae9176fc32dba0f4172dcdb     50\n",
      "50650  0xffff83075509851dca62ca604f191478ff041fd3     50\n",
      "\n",
      "[28833 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# How many users in user-tx dataset have >=50 transactions?\n",
    "temp_df = pd.read_csv(\"dataset/user_transactions.csv\")\n",
    "grouped = temp_df.groupby('address').size().reset_index(name='count')\n",
    "target = grouped[grouped['count'] == 50]\n",
    "print(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50651\n"
     ]
    }
   ],
   "source": [
    "print(len(temp_df['address'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUGFIX: \n",
    "''' If we want to refetch these 28833 addresses transactions, first we need to run the pre-run 4 steps \n",
    "before running the fetch_tx.py script. Then, Run the fetch_tx.py, but update the Step3 to check if we had the from to timestamp\n",
    "skip the tx, otherwise append that. Then run the script to find all unique contract addresses within the new user_tx.csv that were not\n",
    "before in old user_tx.csv. Now run the smart contract downloader and verified_smart_contract to first download the new contracts\n",
    "and then parse them (Now we have the final parquet file of new parsed contracts). Now from parquets we update the \n",
    "contract_addressed_with_names.csv (append new unique ones). EOA addresses are the same but we need to run the EDA for anything related \n",
    "to contracts. Then rerun all recommenders.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
