{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSTALATION #######\n",
    "\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1\n",
    "# !pip uninstall torch-scatter -y\n",
    "# !pip uninstall torch-sparse -y\n",
    "# !pip uninstall pyg-lib -y\n",
    "# !pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y\n",
    "# !pip uninstall sentence_transformers -y\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pyarrow fastparquet\n",
    "!pip install transformers\n",
    "!pip install lightfm\n",
    "!pip install memory-profiler\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install sentence_transformers==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### IMPORT #######\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import pickle\n",
    "from memory_profiler import profile\n",
    "# from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SETUP ARGS ###########\n",
    "possible_experiments = {\n",
    "    0: 'full',\n",
    "    1: 'diversity',\n",
    "    2: 'ucsp',\n",
    "    3: 'icsp',\n",
    "    4: 'usparsity',\n",
    "    5: 'isparsity',\n",
    "    6: 'sBERT',\n",
    "    7: 'TFIDF',\n",
    "    8: 'remove_item_feat', \n",
    "    9: 'add_social_edges',\n",
    "}\n",
    "experiment = possible_experiments[0]\n",
    "\n",
    "possible_modes = ['debug', 'experiment']\n",
    "mode = possible_modes[0]\n",
    "\n",
    "model_variants = ['gnn', 'pop', 'mfn']\n",
    "len_interactions_to_consider = 100000\n",
    "each_user_all2all_new_edges = 10 # when running sBERT, we have the embedding saved for 100k contracts\n",
    "# model_variant_eval = model_variants[1]\n",
    "\n",
    "dataset_mode = 'contract'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    # print(\"Mapping of user IDs to consecutive values:\")\n",
    "    # print(\"==========================================\")\n",
    "    # print(unique_user_id.head())\n",
    "\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    # print(\"Mapping of item IDs to consecutive values:\")\n",
    "    # print(\"===========================================\")\n",
    "    # print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # print()\n",
    "    # print(\"Final edge indices pointing from users to items:\")\n",
    "    # print(\"=================================================\")\n",
    "    # print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "def movie_loader():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "    extract_zip(download_url(url, '.'), '.')\n",
    "    movies_path = './ml-latest-small/movies.csv'\n",
    "    ratings_path = './ml-latest-small/ratings.csv'\n",
    "    items_ratings_df = pd.read_csv(ratings_path)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    items_df = pd.read_csv(movies_path)\n",
    "    items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "    items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "    items_df = items_df.sort_values('mappedID') # (Just the last 20 movies have NaN mappedId)\n",
    "    genres = items_df['genres'].str.get_dummies('|')\n",
    "    print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "    item_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "    assert item_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def movie_loader_sparse(k):\n",
    "    # Load Data\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "    extract_zip(download_url(url, '.'), '.')\n",
    "    movies_path = './ml-latest-small/movies.csv'\n",
    "    ratings_path = './ml-latest-small/ratings.csv'\n",
    "\n",
    "    # Read and rename columns\n",
    "    items_ratings_df = pd.read_csv(ratings_path)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "    items_df = pd.read_csv(movies_path)\n",
    "    items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "\n",
    "    # Select k% of each user's ratings\n",
    "    items_ratings_df = items_ratings_df.groupby('userId').apply(lambda x: x.sample(frac=k/100)).reset_index(drop=True)\n",
    "\n",
    "    # Keep only items present in the filtered ratings\n",
    "    valid_item_ids = items_ratings_df['itemId'].unique()\n",
    "    items_df = items_df[items_df['itemId'].isin(valid_item_ids)]\n",
    "\n",
    "    # Recompute unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "\n",
    "    # Merge and sort items data\n",
    "    items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "    items_df = items_df.sort_values('mappedID')\n",
    "\n",
    "    # Process genres and create item features\n",
    "    genres = items_df['genres'].str.get_dummies('|')\n",
    "    item_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "\n",
    "    # Ensure the item feature size is as expected\n",
    "    assert item_feat.size() == (len(valid_item_ids), 20)  # 20 genres in total.\n",
    "\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df\n",
    "\n",
    "\n",
    "def contract_loader():\n",
    "    items_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "    items_ratings_df = items_ratings_df[:len_interactions_to_consider] if mode == 'debug' else items_ratings_df #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "    def calculate_sparcity_value(df):\n",
    "        num_users = df['user'].nunique()\n",
    "        num_items = df['item'].nunique()\n",
    "        num_interactions = len(df)\n",
    "        total_possible_interactions = num_users * num_items / 100\n",
    "        sparsity = 1 - (num_interactions / total_possible_interactions)\n",
    "        return sparsity\n",
    "    \n",
    "    def filter_interactions(df, column, k):\n",
    "        valid_entries = df[column].value_counts()\n",
    "        valid_entries = valid_entries[valid_entries >= k]\n",
    "        df = df[df[column].isin(valid_entries.index)]\n",
    "        print(f'{column} sparcity value is:', calculate_sparcity_value(df))\n",
    "        return df\n",
    "\n",
    "    ########## SPARCITY EXPERIMENT ###########\n",
    "    if experiment == 'usparsity':\n",
    "        u = 1\n",
    "        items_ratings_df = filter_interactions(items_ratings_df, 'user', u)\n",
    "    elif experiment == 'isparsity':\n",
    "        i = 20\n",
    "        items_ratings_df = filter_interactions(items_ratings_df, 'item', i)\n",
    "\n",
    "    items_df = {}\n",
    "    items_df['name'] = items_ratings_df['item'].unique()\n",
    "    items_df['itemId'], unique_names = pd.factorize(items_df['name'])\n",
    "    # items_df['itemId'] = items_df['itemId'] + 1 #TODO test commenting this line didn't breal anything\n",
    "    items_df = pd.DataFrame(items_df, columns=['itemId', 'name'])\n",
    "\n",
    "    def get_item_feat_sbert(items_df):\n",
    "        contract2comments = pd.read_parquet('dataset/contracts2comment.parquet')\n",
    "        c2c_main_class = contract2comments[contract2comments['contract_name'] == contract2comments['class_name']]\n",
    "\n",
    "        def reorder_text(text):\n",
    "            lines = text.split(\"\\n\")\n",
    "            notice_lines = [line for line in lines if \"@notice\" in line]\n",
    "            other_lines = [line for line in lines if \"@notice\" not in line]\n",
    "            reorderd_text = \"\\n\".join(notice_lines + other_lines)\n",
    "            return reorderd_text\n",
    "\n",
    "        def preprocess_text(text):\n",
    "            text = reorder_text(text)\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            # Remove special characters, numbers, etc.\n",
    "            text = re.sub(r'\\W', ' ', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            text = text[:512] if len(text) > 512 else text\n",
    "            return text\n",
    "\n",
    "        sentences = []\n",
    "        for i, item in items_df.iterrows():\n",
    "            comment_class = c2c_main_class[c2c_main_class['contract_name'] == item['name']]\n",
    "            if not comment_class.empty and comment_class['class_documentation'].iloc[0] != '':\n",
    "                sentences.append(comment_class['class_documentation'].iloc[0])\n",
    "            else:\n",
    "                class_names = contract2comments[contract2comments['contract_name'] == item['name']]['class_name']\n",
    "                sentences.append(' '.join(class_names))\n",
    "\n",
    "        preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else \"cpu\") # NOT enough GPU memory\n",
    "        model = model.to(device)\n",
    "        inputs = tokenizer(preprocessed_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        item_feat = embeddings\n",
    "        # model = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
    "        # embeddings = model.encode(preprocessed_sentences)\n",
    "        \n",
    "        return item_feat\n",
    "    \n",
    "    def get_item_feat_tfidf(items_df):\n",
    "        contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "        contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "        contracts_df_top_words = items_df.merge(contract_top_words_df, on='name', how='left')\n",
    "        contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "        items_df = contracts_df_top_words\n",
    "        items_df.set_index('itemId', inplace=True)\n",
    "        # f =5 # ratio to determine the number of top keywords selected for each contract to construct item_feat\n",
    "        items_df['truncated_keywords'] = items_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "        X_df = items_df['truncated_keywords'].str.get_dummies(',')\n",
    "        item_feat = torch.from_numpy(X_df.values).to(torch.float)\n",
    "        return item_feat\n",
    "    \n",
    "    ########### SBERT EXPERIMENT ###########\n",
    "    if experiment == 'TFIDF':\n",
    "        # item_feat = get_item_feat_tfidf(items_df)\n",
    "        item_feat = np.load('tfidf_embeddings_full.npy') # np.load('tfidf_embeddings_100k.npy')\n",
    "        item_feat = torch.from_numpy(item_feat[:len(items_df['itemId'].unique())]).to(torch.float)\n",
    "        \n",
    "    else:\n",
    "        # item_feat = get_item_feat_sbert(items_df)\n",
    "        item_feat = np.load('sbert_embeddings_full.npy') #np.load('sbert_embeddings_100k.npy')\n",
    "        item_feat = torch.from_numpy(item_feat[:len(items_df['itemId'].unique())]).to(torch.float)\n",
    "        print(item_feat.shape)\n",
    "\n",
    "    print('item feature tensor shape', item_feat.shape)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    print('number of unique users', len(unique_user_id))\n",
    "    print('number of unique items', len(unique_item_id))\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df\n",
    "\n",
    "loaders = {\n",
    "    'contract_loader': contract_loader,\n",
    "    'movie_loader': movie_loader,\n",
    "}\n",
    "#unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df = loaders[f'{dataset_mode}_loader']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(edge_index_user_to_item):\n",
    "    # Determine the number of users and items\n",
    "    num_users = edge_index_user_to_item[0].max() + 1  # Assuming user IDs start from 0\n",
    "    num_items = edge_index_user_to_item[1].max() + 1  # Assuming item IDs start from 0\n",
    "\n",
    "    # Calculate the number of interactions\n",
    "    num_interactions = edge_index_user_to_item.shape[1]\n",
    "\n",
    "    # Total possible interactions\n",
    "    total_possible_interactions = num_users * num_items\n",
    "\n",
    "    # Calculate sparsity\n",
    "    sparsity = 1 - (num_interactions / total_possible_interactions)\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting ./ml-latest-small.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6151\n",
      "49.575409836065575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df = movie_loader_sparse(30)\n",
    "print(len(unique_item_id))\n",
    "print(len(edge_index_user_to_item[0])/len(unique_user_id))\n",
    "calculate_sparsity(edge_index_user_to_item)\n",
    "len(unique_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LINK BINARY PRED MODEL ##########\n",
    "def train_test_generator(unique_user_id, item_feat, edge_index_user_to_item):  \n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(item_feat.shape[0])\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0,\n",
    "        num_test=0.2,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=2,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return data, train_data, test_data\n",
    "\n",
    "def GNN_recommender(data, train_data):\n",
    "\n",
    "    # Define seed edges:\n",
    "    print('1')\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    print('2')\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    print('3')\n",
    "\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to drive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(item_feat.shape[1], hidden_channels)\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## AUC EVAL VALIDATION #########\n",
    "    # edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    # edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # # val_data has neg samples in it\n",
    "    # val_loader = LinkNeighborLoader(\n",
    "    #     data=val_data,\n",
    "    #     num_neighbors=[20, 10],\n",
    "    #     edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "    #     edge_label=edge_label,\n",
    "    #     batch_size=3 * 128,\n",
    "    #     shuffle=False,\n",
    "    # )\n",
    "    # sampled_data = next(iter(val_loader))\n",
    "    # preds = []\n",
    "    # ground_truths = []\n",
    "    # for sampled_data in tqdm(val_loader):\n",
    "    #     with torch.no_grad():\n",
    "    #         sampled_data.to(device)\n",
    "    #         preds.append(model(sampled_data))\n",
    "    #         ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    # pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    # ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    # auc = roc_auc_score(ground_truth, pred)\n",
    "    # print()\n",
    "    # print(f\"Validation AUC: {auc:.4f}\")\n",
    "    # return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TRAIN TEST GENERAION ############\n",
    "\n",
    "####### ITEM FEAT ABLATION EXPRIMENT ####### \n",
    "if experiment == 'ablation_item_feat':\n",
    "    item_feat = torch.zeros_like(item_feat)\n",
    "\n",
    "# ####### SOCIAL EDGES ABLEATION EXPERIMENT #######\n",
    "def add_social_edges(edge_index_user_to_item, unique_user_id, unique_item_id, items_ratings_df, item_feat):\n",
    "    unique_item_id = unique_item_id.copy()\n",
    "    # Define the filename where the data will be saved\n",
    "    filename = 'dataset/saved_social_edges_100k.pkl'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # If it does, load the data and return it\n",
    "        with open(filename, 'rb') as f:\n",
    "            unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat = pickle.load(f)\n",
    "        print('Data loaded from file')\n",
    "    else:\n",
    "        user_transactions_df = pd.read_parquet('dataset/user_transactions.parquet')\n",
    "        contract_addresses = pd.read_parquet('dataset/contract_addresses.parquet')\n",
    "        contract_set = set(contract_addresses['address'])\n",
    "\n",
    "        # Shifting item_ids\n",
    "        edge_index_user_to_item[1] = edge_index_user_to_item[1] + len(edge_index_user_to_item[0].unique())\n",
    "        unique_item_id['mappedID'] = unique_item_id['mappedID'] + len(edge_index_user_to_item[0].unique())\n",
    "        #Adding user_ids to item_ids since now users can be an item too #TODO if the GNN performance turned to be bad, just add 'to' user addresses to both item_feat and unique_item_ids\n",
    "        unique_item_id_w_users = pd.concat([unique_user_id.rename(columns={'userId': 'entityId'}), unique_item_id.rename(columns={'itemId': 'entityId'})], axis=0)\n",
    "        user_feat = torch.zeros((len(edge_index_user_to_item[0].unique()), item_feat.shape[1]))\n",
    "        item_feat= torch.cat([user_feat, item_feat], dim=0) # Why don't we adding item_feat to user_feat?\n",
    "\n",
    "        # unique_item_id_w_users['type'] = 'user' or 'item'\n",
    "\n",
    "        users = items_ratings_df['userId'].unique()\n",
    "\n",
    "        print('edge index shape before adding social edges:', edge_index_user_to_item.shape)\n",
    "        count = 0\n",
    "        #note there is a 200k constraint, delete it\n",
    "        for i, interaction in tqdm(user_transactions_df.iterrows(), total=len(user_transactions_df)):\n",
    "            if interaction['from'] not in contract_set and interaction['to'] not in contract_set and interaction['from'] in users and interaction['to'] in users:\n",
    "                if interaction['from'] == interaction['to']: continue\n",
    "                from_user_id = unique_item_id_w_users[unique_item_id_w_users['entityId'] == interaction['from']]['mappedID'].iloc[0]\n",
    "                to_user_id = unique_item_id_w_users[unique_item_id_w_users['entityId'] == interaction['to']]['mappedID'].iloc[0]\n",
    "                social_edge = torch.tensor([[from_user_id], \n",
    "                                            [to_user_id]], dtype=torch.int64)\n",
    "                edge_index_user_to_item = torch.cat([edge_index_user_to_item, social_edge], dim=1)\n",
    "                # count += 1\n",
    "                # if count % 5 == 0: break\n",
    "        print('edge index shape after adding social edges:', edge_index_user_to_item.shape)\n",
    "        del user_transactions_df\n",
    "        del contract_addresses\n",
    "        del contract_set\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        #uncomment below\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump((unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat), f)\n",
    "        print('social edges saved to dataset/saved_social_edges_100k.pkl')\n",
    "    \n",
    "    return unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat\n",
    "\n",
    "if experiment == 'add_social_edges':\n",
    "    # uncomment below after debuging\n",
    "    unique_user_id_w_social, unique_item_id_w_social, edge_index_user_to_item_w_social, item_feat_w_social = add_social_edges(edge_index_user_to_item, unique_user_id, unique_item_id, items_ratings_df, item_feat)\n",
    "    data, train_data, test_data = train_test_generator(unique_user_id_w_social, item_feat_w_social, edge_index_user_to_item_w_social)\n",
    "else:\n",
    "    data, train_data, test_data = train_test_generator(unique_user_id, item_feat, edge_index_user_to_item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 55.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.5937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.3760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.3396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.3159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 46.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.3042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## GNN TRAINING ############\n",
    "#if model_mode == GNN run below\n",
    "model_gnn = GNN_recommender(data, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CSP EXPRIMENTS #######\n",
    "### CSP #### note: if the ratio==1, rerun from the first step\n",
    "if experiment == 'ucsp' or experiment == 'icsp':\n",
    "    def csp_test_gen(train_data, test_data, unique_data, entity_index, experiment_abbr):\n",
    "        train_data_unique_entities = set(train_data['user', 'rates', 'item'].edge_label_index[entity_index].unique().numpy())\n",
    "        unique_entities = set(unique_data['mappedID'].unique())\n",
    "        entities_not_in_train = unique_entities - train_data_unique_entities\n",
    "        mask = torch.tensor([entity in entities_not_in_train for entity in test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index].numpy()])\n",
    "        \n",
    "        test_data_filtered = copy.deepcopy(test_data)\n",
    "        test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[:, mask]\n",
    "        test_data_filtered[\"user\", \"rates\", \"item\"].edge_label = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label[mask]\n",
    "        \n",
    "        ratio = len(test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[entity_index]) / len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index])\n",
    "        print(f'test to train ratio {experiment_abbr}', ratio)\n",
    "        \n",
    "        return test_data_filtered, ratio\n",
    "\n",
    "    test_data_csp, test_to_train_ratio_csp = csp_test_gen(\n",
    "        train_data, test_data, unique_user_id, 0 if experiment == 'ucsp' else 1, 'CSP-user' if experiment == 'ucsp' else 'CSP-item'\n",
    "    )\n",
    "    print('test data len BEFOR CSP test gen:', len(test_data['user', 'rates', 'item'].edge_label_index[0]))\n",
    "    print('test data len AFTER CSP test gen:', len(test_data_csp['user', 'rates', 'item'].edge_label_index[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:04<00:00, 147.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test edges shape BEFORE adding all possible user item pairs torch.Size([2, 18144])\n",
      "test edges shape AFTER adding all possible user item pairs torch.Size([2, 24854])\n",
      "unique test users 610\n",
      "unique test items 5662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######## ALL_TO_ALL USER_ITEM PAIRS GENERATOR IN TEST_DATA #########\n",
    "\n",
    "# If mode GNN run below\n",
    "### SLICING TEST_DATA FOR ALL_TO_ALL EVAL ###\n",
    "slice_rate = 1\n",
    "if experiment == 'ucsp' or experiment == 'icsp': \n",
    "    slice_rate = 1\n",
    "    test_data_sliced = copy.deepcopy(test_data_csp)\n",
    "else:\n",
    "    test_data_sliced = copy.deepcopy(test_data)\n",
    "\n",
    "test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index = test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[:, : int(slice_rate * len(test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[0]))]\n",
    "test_data_sliced[\"user\", \"rates\", \"item\"].edge_label = test_data_sliced[\"user\", \"rates\", \"item\"].edge_label[ : int(slice_rate * len(test_data_sliced[\"user\", \"rates\", \"item\"].edge_label))]\n",
    "\n",
    "edge_index_zip = set(zip(test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[0].numpy(), test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[1].numpy()))\n",
    "\n",
    "all_users = test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[0].unique().numpy()\n",
    "all_items = test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy()\n",
    "\n",
    "# which elp the model most: keep the social_edges in test and be evaluated or remove all social_edges in test_set?\n",
    "if experiment == 'add_social_edges': all_items = [item for item in all_items if item > len(all_users)]\n",
    "\n",
    "new_edges = []\n",
    "new_labels = []\n",
    "\n",
    "edge_index_set = set(edge_index_zip)\n",
    "for user_id in tqdm(all_users, total=len(all_users)):\n",
    "    count_user_new_edges = 0\n",
    "    random.shuffle(all_items) #TODO: before that we should exclude items that user interacted with (ground_truth) since we add label=0 for all new edges\n",
    "    for item_id in all_items:  #TODO: maybe here first shuffle item_id, to prevent adding same items for all users \n",
    "        if count_user_new_edges > each_user_all2all_new_edges:\n",
    "            break\n",
    "        if (user_id, item_id) not in edge_index_set:\n",
    "            count_user_new_edges += 1\n",
    "            new_edges.append((user_id, item_id))\n",
    "            new_labels.append(0)\n",
    "\n",
    "test_data_all2all = copy.deepcopy(test_data_sliced)\n",
    "if new_edges:\n",
    "    new_edges_tensor = torch.tensor(new_edges, dtype=torch.int64).t().contiguous()\n",
    "    new_labels_tensor = torch.tensor(new_labels, dtype=torch.int64)\n",
    "\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index, new_edges_tensor), dim=1)\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label, new_labels_tensor), dim=0)\n",
    "\n",
    "print('test edges shape BEFORE adding all possible user item pairs', test_data_sliced[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "print('test edges shape AFTER adding all possible user item pairs', test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "\n",
    "print('unique test users', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[0].unique()))\n",
    "print('unique test items', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[1].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 146.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ground truth len 24854\n",
      "required time for GNN inference on testset 0.46135830879211426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######## GNN PRED FOR TEST_DATA_all2all ######### \n",
    "import time\n",
    "\n",
    "def pred_gnn_gen(device, model_gnn, test_data):\n",
    "    test_loader_gnn = LinkNeighborLoader(\n",
    "        data=test_data_all2all,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index),\n",
    "        edge_label=test_data_all2all[\"user\", \"rates\", \"item\"].edge_label,\n",
    "        batch_size= 3 * 128, # TO calculate latency on inference time, use 17 as batch size, this will yield 10400 predictions that is near to MF number of preds (=10600)\n",
    "        shuffle=False,\n",
    "    )\n",
    "    sampled_data_gnn = next(iter(test_loader_gnn))\n",
    "    preds_gnn = []\n",
    "    ground_truths_gnn = []\n",
    "    model_gnn = model_gnn.to(device)\n",
    "    for sampled_data_gnn in tqdm(test_loader_gnn):\n",
    "        with torch.no_grad():\n",
    "            sampled_data_gnn.to(device)\n",
    "            preds_gnn.append(model_gnn(sampled_data_gnn))\n",
    "            ground_truths_gnn.append(sampled_data_gnn[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    pred_gnn = torch.cat(preds_gnn, dim=0).cpu().numpy()\n",
    "    ground_truth_gnn = torch.cat(ground_truths_gnn, dim=0).cpu().numpy()\n",
    "    print('all ground truth len', len(ground_truth_gnn))\n",
    "    return pred_gnn, ground_truth_gnn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # To check the memory usage of GNN, put cpu to be comparable with MF models\n",
    "start_time = time.time()\n",
    "pred_gnn, ground_truth_gnn = pred_gnn_gen(device, model_gnn, test_data_all2all)\n",
    "end_time = time.time()\n",
    "print('required time for GNN inference on testset', end_time - start_time)\n",
    "# %memit pred_gnn_gen(device, test_data_all2all) %Uncomment to check the memory usage of GNN on inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR MF & POP MODELS  #############\n",
    "'''\n",
    "For LightFM models, we need a df of train and test data, \n",
    "but from GNN train/test generation, we have a HeteroData\n",
    "Here we turn a HeteroData to a DataFrame\n",
    "'''\n",
    "\n",
    "def add_topic(df, contract_to_topic_df, unique_item_id):\n",
    "    item_to_topic = pd.Series(contract_to_topic_df['most_probable_topic'].values, index=contract_to_topic_df['contract_name']).to_dict()\n",
    "    mappedID_to_itemId = pd.Series(unique_item_id['itemId'].values, index=unique_item_id['mappedID']).to_dict()\n",
    "    df['item_name'] = df['item'].map(mappedID_to_itemId)\n",
    "    df['topic'] = df['item_name'].map(item_to_topic).fillna(0).astype(int)\n",
    "    df = df.drop(columns=['item_name'])\n",
    "    return df\n",
    "\n",
    "test_df_index = test_data_all2all['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "test_df_label = test_data_all2all['user', 'rates', 'item'].edge_label.numpy()\n",
    "\n",
    "test_df_index = test_df_index.T \n",
    "test_df_mf = pd.DataFrame(test_df_index, columns=['user', 'item'])\n",
    "test_df_mf['rating'] = test_df_label\n",
    "\n",
    "\n",
    "train_df_index = train_data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_df_label = train_data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_df_index = train_df_index.T \n",
    "train_df_mf = pd.DataFrame(train_df_index, columns=['user', 'item'])\n",
    "train_df_mf['rating'] = train_df_label\n",
    "\n",
    "if dataset_mode == 'contract':\n",
    "    contract_to_topic_df = pd.read_parquet(\"dataset/contract_name_topic.parquet\")\n",
    "    train_df_mf= add_topic(train_df_mf, contract_to_topic_df, unique_item_id)\n",
    "    test_df_mf = add_topic(test_df_mf, contract_to_topic_df, unique_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 977.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required time for MFN inference on testset with neg_edges_param = 10: 0.6301472187042236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### POP & MF_N & MF_C TRAIN/PRED  #############\n",
    "top_contracts = train_df_mf['item'].value_counts()[:20].index.tolist() # can put any number > max(k) instead of 100\n",
    "test_df_mf['pred_pop'] = 0\n",
    "test_df_mf.loc[test_df_mf['item'].isin(top_contracts), 'pred_pop'] = 1\n",
    "pred_pop = test_df_mf['pred_pop'].to_numpy()\n",
    "ground_truth_pop = test_df_mf['rating'].to_numpy()\n",
    "\n",
    "##### MF_N #####\n",
    "def mfn_pred_gen(train_df_mf, test_df_mf):\n",
    "    dataset = Dataset()\n",
    "    user_ids_mfn = np.union1d(train_df_mf['user'].unique(), test_df_mf['user'].unique())\n",
    "    item_ids_mfn = np.union1d(train_df_mf['item'].unique(), test_df_mf['item'].unique())\n",
    "    dataset.fit(user_ids_mfn, item_ids_mfn)\n",
    "    user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "    (train_interactions_mfn, train_interactions_weight_mfn) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_df_mf.iterrows())\n",
    "\n",
    "    model_mfn = LightFM(loss='warp')\n",
    "    model_mfn.fit(train_interactions_mfn, epochs=30, num_threads=2)\n",
    "\n",
    "    test_df_mf['pred_mfn'] = float(0)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for user, user_data in tqdm(test_df_mf.groupby('user'), total=test_df_mf['user'].nunique()):\n",
    "        user_id_internal = user_ids_mapping[user]\n",
    "        item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['item']])\n",
    "        predictions_mfn = model_mfn.predict(user_id_internal, item_ids_internal)\n",
    "        test_df_mf.loc[user_data.index, 'pred_mfn'] = predictions_mfn\n",
    "    end_time = time.time()\n",
    "    print('required time for MFN inference on testset with neg_edges_param = 10:', end_time - start_time)\n",
    "\n",
    "    pred_mfn = test_df_mf['pred_mfn'].to_numpy()\n",
    "    ground_truth_mfn = test_df_mf['rating'].to_numpy()\n",
    "    return pred_mfn, ground_truth_mfn\n",
    "\n",
    "pred_mfn, ground_truth_mfn = mfn_pred_gen(train_df_mf, test_df_mf)\n",
    "\n",
    "##### MF_C #####\n",
    "if dataset_mode == 'contract':\n",
    "#     def mfc_pred_gen(train_df_mf, test_df_mf):\n",
    "#         dataset = Dataset()\n",
    "#         user_ids_mfc = np.union1d(train_df_mf['user'].unique(), test_df_mf['user'].unique())\n",
    "#         item_ids_mfc = np.union1d(train_df_mf['topic'].unique(), test_df_mf['topic'].unique())\n",
    "#         dataset.fit(user_ids_mfc, item_ids_mfc)\n",
    "#         user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "#         (train_interactions_mfc, train_interactions_weight_mfc) = dataset.build_interactions((row['user'], row['topic'], row['rating']) for index, row in train_df_mf.iterrows())\n",
    "\n",
    "#         model_mfc = LightFM(loss='warp')\n",
    "#         model_mfc.fit(train_interactions_mfc, epochs=30, num_threads=2, sample_weight=train_interactions_weight_mfc) # TODO maybe do not pass the weights to the MF models\n",
    "\n",
    "#         def topic_popular_contracts(df):\n",
    "#             item_rating_sum = df.groupby(['topic', 'item'])['rating'].sum().reset_index()\n",
    "#             sorted_items = item_rating_sum.sort_values(['topic', 'rating'], ascending=[True, False])\n",
    "#             topic_to_popular_items = {k: g['item'].tolist() for k, g in sorted_items.groupby('topic')}\n",
    "#             return topic_to_popular_items\n",
    "\n",
    "#         test_df_mf['pred_mfc'] = float(0)\n",
    "#         topic_popular_contracts_dict = topic_popular_contracts(test_df_mf)\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         for user, user_data in tqdm(test_df_mf.groupby('user'), total=test_df_mf['user'].nunique()):\n",
    "#             user_id_internal = user_ids_mapping[user]\n",
    "#             item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['topic']])\n",
    "#             predictions_mfc = model_mfc.predict(user_id_internal, item_ids_internal)\n",
    "#             test_df_mf.loc[user_data.index, 'pred_mfc'] = predictions_mfc\n",
    "#         end_time = time.time()\n",
    "#         print('required time for MFC inference on testset with neg_edges_param = 10:', end_time - start_time)\n",
    "\n",
    "#         pred_mfc = test_df_mf['pred_mfc'].to_numpy()\n",
    "#         ground_truth_mfc = test_df_mf['rating'].to_numpy()\n",
    "#         return pred_mfc, ground_truth_mfc\n",
    "\n",
    "    # pred_mfc, ground_truth_mfc = mfc_pred_gen(train_df_mf, test_df_mf)\n",
    "\n",
    "    def mfc_pred_gen(train_df_mf, test_df_mf):\n",
    "        dataset = Dataset()\n",
    "        user_ids_mfc = np.union1d(train_df_mf['user'].unique(), test_df_mf['user'].unique())\n",
    "        item_ids_mfc = np.union1d(train_df_mf['item'].unique(), test_df_mf['item'].unique())\n",
    "        topic_ids = np.union1d(train_df_mf['topic'].unique(), test_df_mf['topic'].unique())\n",
    "\n",
    "        # We need to tell the dataset about the item feature columns we have:\n",
    "        dataset.fit(\n",
    "            users=user_ids_mfc, \n",
    "            items=item_ids_mfc,\n",
    "            item_features=topic_ids,\n",
    "        )\n",
    "        user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "        # Building item features such that each item has its associated topic as a feature\n",
    "        item_features = dataset.build_item_features(\n",
    "            (x, [y]) for x,y in zip(train_df_mf['item'], train_df_mf['topic'])\n",
    "        )\n",
    "\n",
    "        (train_interactions_mfn, train_interactions_weight_mfn) = dataset.build_interactions(\n",
    "            (row['user'], row['item'], row['rating']) for index, row in train_df_mf.iterrows()\n",
    "        )\n",
    "\n",
    "        model_mfc = LightFM(loss='warp')\n",
    "        # Including the item features in the fit method\n",
    "        model_mfc.fit(\n",
    "            train_interactions_mfn, \n",
    "            item_features=item_features, \n",
    "            epochs=30, \n",
    "            num_threads=2\n",
    "        )\n",
    "        test_df_mf['pred_mfc'] = float(0)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for user, user_data in tqdm(test_df_mf.groupby('user'), total=test_df_mf['user'].nunique()):\n",
    "            user_id_internal = user_ids_mapping[user]\n",
    "            item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['item']])\n",
    "            predictions_mfn = model_mfc.predict(user_id_internal, item_ids_internal)\n",
    "            test_df_mf.loc[user_data.index, 'pred_mfc'] = predictions_mfn\n",
    "        end_time = time.time()\n",
    "        print('required time for MFC inference on testset with neg_edges_param = 10:', end_time - start_time)\n",
    "\n",
    "        pred_mfn = test_df_mf['pred_mfc'].to_numpy()\n",
    "        ground_truth_mfn = test_df_mf['rating'].to_numpy()\n",
    "        return pred_mfn, ground_truth_mfn\n",
    "\n",
    "    pred_mfc, ground_truth_mfc = mfc_pred_gen(train_df_mf, test_df_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$ gnn $$$$$$\n",
      "HIT@1: 0.5475409836065573\n",
      "HIT@5: 0.8688524590163934\n",
      "HIT@10: 0.9229508196721311\n",
      "HIT@15: 0.9393442622950819\n",
      "HIT@20: 0.9459016393442623\n",
      "NDCG@1: 0.5475409836065573\n",
      "NDCG@5: 0.5676712411083231\n",
      "NDCG@10: 0.6066840496623556\n",
      "NDCG@15: 0.637545447173939\n",
      "NDCG@20: 0.6572792294492733\n",
      "MAP@1: 0.5475409836065573\n",
      "MAP@5: 0.6400455373406193\n",
      "MAP@10: 0.609270642303673\n",
      "MAP@15: 0.5855408967910019\n",
      "MAP@20: 0.5722937223394562\n"
     ]
    }
   ],
   "source": [
    "####### METRIC EVAL #######\n",
    "\n",
    "# def precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "#     top_k_indices = sorted_indices[:k]\n",
    "#     top_k_labels = ground_truth[top_k_indices]\n",
    "#     num_ones = np.sum(ground_truth == 1)\n",
    "#     hit = np.sum(top_k_labels > 0)\n",
    "\n",
    "#     return hit / min(num_ones, k) if num_ones != 0 else k # k\n",
    "\n",
    "# def average_hit_at_k(k, ground_truth, pred, user_ids, edge_index, model_variant):\n",
    "#     precisions = []\n",
    "#     for user_id in user_ids: # tqdm(user_ids, total=len(user_ids)):\n",
    "#         mask = edge_index[0] == user_id\n",
    "#         filtered_pred = pred[mask]\n",
    "#         filtered_ground_truth = ground_truth[mask]\n",
    "#         if np.sum(filtered_ground_truth == 1) == 0: continue\n",
    "#         # print(filtered_pred)\n",
    "#         # print(filtered_ground_truth)\n",
    "#         sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "#         pop_hit = np.sum(filtered_pred[:np.sum(filtered_ground_truth == 1)] > 0) / (min(np.sum(filtered_ground_truth == 1), k) if np.sum(filtered_ground_truth == 1) != 0 else k)\n",
    "        \n",
    "#         precisions.append(\n",
    "#             precision_at_k(user_id, sorted_indices, filtered_ground_truth, k) if model_variant != 'pop' else pop_hit\n",
    "#         )\n",
    "#         break\n",
    "        \n",
    "#     return np.mean(precisions)\n",
    "\n",
    "def precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Computes the hit@k for a single user.\n",
    "\n",
    "    Args:\n",
    "    user_id: The user id.\n",
    "    sorted_indices: Indices that would sort the predicted ratings.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    k: The number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    The hit@k for the given user.\n",
    "    \"\"\"\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_labels = ground_truth[top_k_indices]\n",
    "    \n",
    "    # Check if there's any relevant item in the top k recommendations\n",
    "    hit = int(np.sum(top_k_labels) > 0)\n",
    "\n",
    "    return hit\n",
    "\n",
    "def average_hit_at_k(k, ground_truth, pred, user_ids, edge_index, model_variant):\n",
    "    \"\"\"\n",
    "    Computes the mean hit@k.\n",
    "\n",
    "    Args:\n",
    "    k: The number of recommendations to consider.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    pred: Predicted ratings.\n",
    "    user_ids: Array of user ids to calculate the metric for.\n",
    "\n",
    "    Returns:\n",
    "    The mean hit@k over all users.\n",
    "    \"\"\"\n",
    "    \n",
    "    hits = []\n",
    "    for user_id in user_ids: \n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        pop_hit = np.sum(filtered_pred[:np.sum(filtered_ground_truth == 1)] > 0) / (min(np.sum(filtered_ground_truth == 1), k) if np.sum(filtered_ground_truth == 1) != 0 else k)\n",
    "        hits.append(precision_at_k(user_id, sorted_indices, filtered_ground_truth, k) if model_variant != 'pop' else pop_hit)\n",
    "        \n",
    "    return np.mean(hits)\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute DCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - DCG@k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - NDCG@k\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "def calculate_ndcg_at_k(k, ground_truth, pred, edge_index):\n",
    "    \"\"\"\n",
    "    Calculate the average NDCG@k for all users\n",
    "    \n",
    "    Parameters:\n",
    "    - k: Rank\n",
    "    - ground_truth: True relevance scores\n",
    "    - pred: Predicted relevance scores\n",
    "    - edge_index: User-item interaction indices\n",
    "    \n",
    "    Returns:\n",
    "    - Average NDCG@k\n",
    "    \"\"\"\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "    ndcgs = []\n",
    "    for user_id in user_ids: # tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        \n",
    "        # Sort by predicted score\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        sorted_ground_truth = filtered_ground_truth[sorted_indices]\n",
    "        \n",
    "        ndcgs.append(ndcg_at_k(sorted_ground_truth, k))\n",
    "        \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "def average_precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single user.\n",
    "    \n",
    "    Args:\n",
    "    user_id: The user id.\n",
    "    sorted_indices: Indices that would sort the predicted ratings.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    k: The number of recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "    The average precision at k for the given user.\n",
    "    \"\"\"\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_labels = ground_truth[top_k_indices]\n",
    "    \n",
    "    relevant_indices = np.where(top_k_labels > 0)[0]\n",
    "    num_relevant = len(relevant_indices)\n",
    "    \n",
    "    if num_relevant == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0.0\n",
    "    for i in relevant_indices:\n",
    "        prec_at_i = np.sum(top_k_labels[:i+1]) / (i + 1)\n",
    "        score += prec_at_i\n",
    "    \n",
    "    return score / min(num_relevant, k)\n",
    "\n",
    "def mean_ap_at_k(k, ground_truth, pred, user_ids, edge_index):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    Args:\n",
    "    k: The number of recommendations to consider.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    pred: Predicted ratings.\n",
    "    \n",
    "    Returns:\n",
    "    The mean average precision at k over all users.\n",
    "    \"\"\"\n",
    "    \n",
    "    average_precisions = []\n",
    "    for user_id in user_ids: # tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        \n",
    "        average_precisions.append(\n",
    "            average_precision_at_k(user_id, sorted_indices, filtered_ground_truth, k)\n",
    "        )\n",
    "        \n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "def evaluate(k_values, test_data_all2all, ground_truth, pred, model_variant_eval):\n",
    "    edge_index = test_data_all2all['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "\n",
    "    for k in k_values:\n",
    "        ### HIT@K ###\n",
    "        hit_at_k = average_hit_at_k(k, ground_truth, pred, user_ids, edge_index, model_variant_eval)\n",
    "        print(f\"HIT@{k}: {hit_at_k}\")\n",
    "    if model_variant_eval != 'pop':\n",
    "        for k in k_values:\n",
    "            ### NDCG@K ###\n",
    "            ndcg_result = calculate_ndcg_at_k(k, ground_truth, pred, edge_index)\n",
    "            print(f\"NDCG@{k}: {ndcg_result}\")\n",
    "        for k in k_values:\n",
    "            map_at_k = mean_ap_at_k(k, ground_truth, pred, user_ids, edge_index)\n",
    "            print(f\"MAP@{k}: {map_at_k}\")\n",
    "\n",
    "\n",
    "eval_loader = {\n",
    "    'gnn': {\n",
    "        'ground_truth': ground_truth_gnn,\n",
    "        'pred': pred_gnn\n",
    "    },\n",
    "    #'pop': {\n",
    "     #   'ground_truth': ground_truth_pop,\n",
    "      #  'pred': pred_pop\n",
    "    #},\n",
    "    #'mfn': {\n",
    "    #    'ground_truth': ground_truth_mfn,\n",
    "    #    'pred': pred_mfn\n",
    "    #},\n",
    "    # 'mfc': {\n",
    "    #     'ground_truth': ground_truth_mfc,\n",
    "    #     'pred': pred_mfc\n",
    "    # },\n",
    "\n",
    "}\n",
    "model_variants = ['gnn']\n",
    "\n",
    "for model_variant_eval in model_variants:\n",
    "    k_values = [1, 5, 10, 15, 20] if mode != 'debug' else [1, 5, 10, 15, 20]\n",
    "    print(f'$$$$$$ {model_variant_eval} $$$$$$')\n",
    "    evaluate(k_values, test_data_all2all, ground_truth=eval_loader[model_variant_eval]['ground_truth'], pred=eval_loader[model_variant_eval]['pred'], model_variant_eval=model_variant_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'diversity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item coverage diversity for mfn @1: 0.18333537857405816\n",
      "Item coverage diversity for mfn @5: 0.5421524113388145\n",
      "Item coverage diversity for mfn @10: 0.9469873604123206\n",
      "Item coverage diversity for mfn @15: 0.99987728555651\n",
      "Item coverage diversity for mfn @20: 1.0\n",
      "User coverage for mfn @1: 0.5025765951466317\n",
      "User coverage for mfn @5: 0.5920547175114775\n",
      "User coverage for mfn @10: 0.6301883256816265\n",
      "User coverage for mfn @15: 0.6531434460788906\n",
      "User coverage for mfn @20: 0.6569849152065961\n",
      "Item coverage diversity for mfc @1: 0.19830654067983802\n",
      "Item coverage diversity for mfc @5: 0.6280525217818137\n",
      "Item coverage diversity for mfc @10: 0.9240397594796907\n",
      "Item coverage diversity for mfc @15: 0.999938642778255\n",
      "Item coverage diversity for mfc @20: 1.0\n",
      "User coverage for mfc @1: 0.4732502576595147\n",
      "User coverage for mfc @5: 0.5725662887660452\n",
      "User coverage for mfc @10: 0.6233486367469315\n",
      "User coverage for mfc @15: 0.6515506418064274\n",
      "User coverage for mfc @20: 0.6568912208376276\n",
      "Item coverage diversity for pop @1: 0.34439808565468155\n",
      "Item coverage diversity for pop @5: 0.9464965026383605\n",
      "Item coverage diversity for pop @10: 0.998097926125905\n",
      "Item coverage diversity for pop @15: 1.0\n",
      "Item coverage diversity for pop @20: 1.0\n",
      "User coverage for pop @1: 0.3433898622692776\n",
      "User coverage for pop @5: 0.48777288484962056\n",
      "User coverage for pop @10: 0.48777288484962056\n",
      "User coverage for pop @15: 0.5947718542115619\n",
      "User coverage for pop @20: 0.655860582778975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nItem coverage diversity for gnn @1: 0.1974024375230826\\nItem coverage diversity for gnn @5: 0.6526529607287948\\nItem coverage diversity for gnn @10: 0.9267512002954573\\nItem coverage diversity for gnn @15: 0.9996922319340146\\nItem coverage diversity for gnn @20: 1.0\\nUser coverage for gnn @1: 0.5559827456864216\\nUser coverage for gnn @5: 0.6452550637659414\\nUser coverage for gnn @10: 0.6614778694673669\\nUser coverage for gnn @15: 0.6639159789947486\\nUser coverage for gnn @20: 0.6641035258814704\\nItem coverage diversity for mfn @1: 0.18576880462883172\\nItem coverage diversity for mfn @5: 0.5387172227009726\\nItem coverage diversity for mfn @10: 0.9447864089622061\\nItem coverage diversity for mfn @15: 0.9996922319340146\\nItem coverage diversity for mfn @20: 0.999938446386803\\nUser coverage for mfn @1: 0.5041260315078769\\nUser coverage for mfn @5: 0.5982745686421606\\nUser coverage for mfn @10: 0.6395348837209303\\nUser coverage for mfn @15: 0.6611965491372843\\nUser coverage for mfn @20: 0.6641035258814704\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# DIVERSITY EXPERIMENT ##############\n",
    "model_variants = ['mfn', 'mfc', 'pop']\n",
    "eval_loader = {\n",
    "    # 'gnn': {\n",
    "    #     'ground_truth': ground_truth_gnn,\n",
    "    #     'pred': pred_gnn\n",
    "    # },\n",
    "    'pop': {\n",
    "        'ground_truth': ground_truth_pop,\n",
    "        'pred': pred_pop\n",
    "    },\n",
    "    'mfn': {\n",
    "        'ground_truth': ground_truth_mfn,\n",
    "        'pred': pred_mfn\n",
    "    },\n",
    "    'mfc': {\n",
    "        'ground_truth': ground_truth_mfc,\n",
    "        'pred': pred_mfc\n",
    "    },\n",
    "\n",
    "}\n",
    "for model_variant_eval in model_variants:\n",
    "    k_values = [1, 5, 10, 15, 20] if mode != 'debug' else [1, 5, 10, 15, 20]\n",
    "if experiment == 'diversity':\n",
    "    edge_index = test_data_all2all['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "\n",
    "    for model_variant_eval in model_variants:\n",
    "        pred = eval_loader[model_variant_eval]['pred']\n",
    "        ground_truth = eval_loader[model_variant_eval]['ground_truth']\n",
    "\n",
    "        for k in k_values:\n",
    "            recs_list = set()\n",
    "            for user_id in user_ids: # tqdm(user_ids, total=len(user_ids)):\n",
    "                mask = edge_index[0] == user_id\n",
    "                filtered_pred = pred[mask]\n",
    "                filtered_items = edge_index[1][mask]\n",
    "                sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "                top_k_indices = sorted_indices[:k]\n",
    "                top_k_indices = top_k_indices.copy()\n",
    "                top_k_items = filtered_items[top_k_indices].numpy()\n",
    "                recs_list.update(top_k_items)\n",
    "\n",
    "            diversity_at_k = len(recs_list) / len(np.unique(edge_index[1].numpy()))\n",
    "            print(f'Item coverage diversity for {model_variant_eval} @{k}:', diversity_at_k)\n",
    "        \n",
    "        for k in k_values:\n",
    "            users_with_relevant_recs = set()\n",
    "            \n",
    "            for user_id in user_ids: # tqdm(user_ids, total=len(user_ids)):\n",
    "                mask = edge_index[0] == user_id\n",
    "                filtered_pred = pred[mask]\n",
    "                sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "                top_k_indices = sorted_indices[:k]\n",
    "                filtered_ground_truth = ground_truth[mask] \n",
    "                relevant_recs = filtered_ground_truth[top_k_indices] \n",
    "                \n",
    "                if np.sum(relevant_recs) > 0:  # At least one relevant recommendation\n",
    "                    users_with_relevant_recs.add(user_id)\n",
    "            \n",
    "            user_coverage_at_k = len(users_with_relevant_recs) / len(user_ids)\n",
    "            print(f'User coverage for {model_variant_eval} @{k}:', user_coverage_at_k)\n",
    "\n",
    "    #######  Intra-List Diversity #######\n",
    "    # TODO: Based on item_feat define the compute_dissimilarity method\n",
    "    # for k in k_values:\n",
    "    #     avg_dissimilarity = []\n",
    "        \n",
    "    #     for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "    #         mask = edge_index[0] == user_id\n",
    "    #         filtered_pred = pred[mask]\n",
    "    #         filtered_items = edge_index[1][mask]\n",
    "    #         sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "    #         top_k_indices = sorted_indices[:k]\n",
    "    #         top_k_items = filtered_items[top_k_indices].numpy()\n",
    "            \n",
    "    #         dissimilarity_sum = 0\n",
    "    #         for i in range(len(top_k_items)):\n",
    "    #             for j in range(i+1, len(top_k_items)):\n",
    "    #                 dissimilarity_sum += compute_dissimilarity(top_k_items[i], top_k_items[j])\n",
    "            \n",
    "    #         if k > 1:\n",
    "    #             avg_pairwise_dissimilarity = 2 * dissimilarity_sum / (k * (k - 1))\n",
    "    #             avg_dissimilarity.append(avg_pairwise_dissimilarity)\n",
    "        \n",
    "    #     intra_list_diversity_at_k = np.mean(avg_dissimilarity)\n",
    "    #     print(f'Intra-list diversity for {model_mode_eval} @{k}:', intra_list_diversity_at_k)\n",
    "\n",
    "'''\n",
    "Item coverage diversity for gnn @1: 0.1974024375230826\n",
    "Item coverage diversity for gnn @5: 0.6526529607287948\n",
    "Item coverage diversity for gnn @10: 0.9267512002954573\n",
    "Item coverage diversity for gnn @15: 0.9996922319340146\n",
    "Item coverage diversity for gnn @20: 1.0\n",
    "User coverage for gnn @1: 0.5559827456864216\n",
    "User coverage for gnn @5: 0.6452550637659414\n",
    "User coverage for gnn @10: 0.6614778694673669\n",
    "User coverage for gnn @15: 0.6639159789947486\n",
    "User coverage for gnn @20: 0.6641035258814704\n",
    "Item coverage diversity for mfn @1: 0.18576880462883172\n",
    "Item coverage diversity for mfn @5: 0.5387172227009726\n",
    "Item coverage diversity for mfn @10: 0.9447864089622061\n",
    "Item coverage diversity for mfn @15: 0.9996922319340146\n",
    "Item coverage diversity for mfn @20: 0.999938446386803\n",
    "User coverage for mfn @1: 0.5041260315078769\n",
    "User coverage for mfn @5: 0.5982745686421606\n",
    "User coverage for mfn @10: 0.6395348837209303\n",
    "User coverage for mfn @15: 0.6611965491372843\n",
    "User coverage for mfn @20: 0.6641035258814704\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
